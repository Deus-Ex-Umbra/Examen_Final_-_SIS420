{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2B8dH345C5e"
      },
      "source": [
        "***Pregunta N°1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNan3X3GNTSw"
      },
      "source": [
        "**Declaración de las Librerías.** Primeramente, es necesario declarar cada una de las librerías a usar, así nos enfocaremos cómodamente en el algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mwUvY1-NSy_",
        "outputId": "a6d324d4-7ddd-43ca-aa6f-0200fbab9eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import optimize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algoritmo K-means-**"
      ],
      "metadata": {
        "id": "AAlsvbJkHOyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funciones para carga y procesamiento del dataset.**"
      ],
      "metadata": {
        "id": "l2E7wAIQME11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jI2XNQw4XfZr"
      },
      "outputs": [],
      "source": [
        "def cargarDataset(_dataset):\n",
        "    dataset = pd.read_csv(_dataset, sep=',', header=0, decimal='.')\n",
        "    datos = {}\n",
        "    columnas = dataset.columns[dataset.dtypes == 'object'].tolist()\n",
        "    for columna in columnas:\n",
        "        datos[columna] = dataset[columna].drop_duplicates().values\n",
        "    datos_num = {}\n",
        "    for columna, valores in datos.items():\n",
        "        indice_reemp = 0\n",
        "        datos_num_col = {}\n",
        "        for valor in valores:\n",
        "            if valor not in datos_num_col and not pd.isnull(valor):\n",
        "                datos_num_col[valor] = indice_reemp\n",
        "                indice_reemp += 1\n",
        "        if np.nan not in datos_num_col:\n",
        "            datos_num_col[np.nan] = 0\n",
        "        datos_num[columna] = datos_num_col\n",
        "    for columna, d_n in datos_num.items():\n",
        "        dataset[columna] = dataset[columna].replace(d_n)\n",
        "    dataset = dataset.fillna(0)\n",
        "    return dataset.apply(pd.to_numeric, errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizarCaracteristicas(X):\n",
        "      media = np.mean(X, axis=0)\n",
        "      desviacion_estandar = np.std(X, axis=0)\n",
        "      desviacion_estandar[desviacion_estandar == 0] = 1\n",
        "      X_norm = (X - media) / desviacion_estandar\n",
        "      return X_norm"
      ],
      "metadata": {
        "id": "D6jccNcwXWV9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funciones para las respectivas gráficas.**"
      ],
      "metadata": {
        "id": "7fvCiYvlMLWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pGvstLTrYTDn"
      },
      "outputs": [],
      "source": [
        "def graficarFronterasDecision(clusterizador, X, caracteristicas=[7, 8], resolucion=1000, mostrar_centroides=True):\n",
        "    X_vis = X[:, caracteristicas]\n",
        "    minimos = X_vis.min(axis=0) - 1\n",
        "    maximos = X_vis.max(axis=0) + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(minimos[0], maximos[0], resolucion),\n",
        "                         np.linspace(minimos[1], maximos[1], resolucion))\n",
        "    malla = np.c_[xx.ravel(), yy.ravel()]\n",
        "    malla_completa = np.zeros((len(malla), clusterizador.n_features_in_))\n",
        "    malla_completa[:, caracteristicas] = malla\n",
        "    Z = clusterizador.predict(malla_completa)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.5, cmap='Pastel2')\n",
        "    plt.contour(xx, yy, Z, linewidths=1, colors='k')\n",
        "    plt.scatter(X_vis[:, 0], X_vis[:, 1], c=clusterizador.labels_, s=1)\n",
        "    graficarDatos(X)\n",
        "    if mostrar_centroides:\n",
        "        graficarCentroides(clusterizador.cluster_centers_[:, caracteristicas])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def graficarCentroides(centroides, color_circulo='w', color_cruz='k'):\n",
        "    plt.scatter(centroides[:, 0], centroides[:, 1],\n",
        "                marker='o', s=10, linewidths=8,\n",
        "                color=color_circulo, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroides[:, 0], centroides[:, 1],\n",
        "                marker='x', s=2, linewidths=10,\n",
        "                color=color_cruz, zorder=11, alpha=1)"
      ],
      "metadata": {
        "id": "4T0Oh6tVjS05"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graficarDatos(X, c1 = 7, c2 = 8):\n",
        "    plt.plot(X[:, c1], X[:, c2], 'k.', markersize=2)"
      ],
      "metadata": {
        "id": "ZJ_s_SjHjT3h"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graficarClusters(X, y=None, c1 = 7, c2 = 8):\n",
        "    plt.scatter(X[:, c1], X[:, c2], c=y, s=1)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
      ],
      "metadata": {
        "id": "khYvdPzDjXV3"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Función para encontrar el mejor número posible de grupos.** La puntuación de Silhouette se utiliza como medida de la calidad de los clusters, donde valores más altos indican una mejor separación entre los clusters."
      ],
      "metadata": {
        "id": "rgrP1roZMSzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encontrar_numero_optimo_grupos(X, n=10):\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    kmeans_por_k = [MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=10, n_init=10).fit(X) for k in range(1, n + 1)]\n",
        "    puntuaciones_silhouette = [silhouette_score(X, modelo.labels_) for modelo in kmeans_por_k[1:]]\n",
        "    k_optimo = np.argmax(puntuaciones_silhouette) + 2\n",
        "    print(\"Gráfico de la puntuación de Silhouette\")\n",
        "    plt.figure(figsize=(8, 3))\n",
        "    plt.plot(range(0, n - 1), puntuaciones_silhouette, \"bo-\")\n",
        "    plt.xlabel(\"$k$\", fontsize=14)\n",
        "    plt.ylabel(\"Silhouette score\", fontsize=14)\n",
        "    plt.axis([0, n, -1, 1])\n",
        "    plt.show()\n",
        "    print(\"Gráficos de los coeficientes de Silhouette por cluster\")\n",
        "    plt.figure(figsize=(n + 1, 10))\n",
        "    for k in range(2, n + 1):\n",
        "        plt.subplot(3, 3, k - 1)\n",
        "        y_pred = kmeans_por_k[k - 1].labels_\n",
        "        silhouette_coefficients = silhouette_samples(X, y_pred)\n",
        "        padding = len(X) // 30\n",
        "        pos = padding\n",
        "        ticks = []\n",
        "        for i in range(k):\n",
        "            coeffs = silhouette_coefficients[y_pred == i]\n",
        "            coeffs.sort()\n",
        "            color = mpl.cm.Spectral(i / k)\n",
        "            plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
        "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
        "            ticks.append(pos + len(coeffs) // 2)\n",
        "            pos += len(coeffs) + padding\n",
        "        plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
        "        plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
        "        if k in (3, 5):\n",
        "            plt.ylabel(\"Cluster\")\n",
        "        if k in (5, 6):\n",
        "            plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "            plt.xlabel(\"Silhouette Coefficient\")\n",
        "        else:\n",
        "            plt.tick_params(labelbottom=False)\n",
        "        plt.axvline(x=puntuaciones_silhouette[k - 2], color=\"red\", linestyle=\"--\")\n",
        "        plt.title(\"$k={}$\".format(k), fontsize=16)\n",
        "\n",
        "    plt.show()\n",
        "    return k_optimo"
      ],
      "metadata": {
        "id": "mC11nsgw0MbN"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Este bloque de código carga el conjunto de datos, determina el número óptimo de clusters, aplica el algoritmo KMeans para encontrar los clusters óptimos, grafica los datos originales con los clusters encontrados y muestra las gráficas. Finalmente, asigna las etiquetas de los clusters al conjunto de datos para su posterior análisis.*"
      ],
      "metadata": {
        "id": "HgpHHQonMcEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = cargarDataset('/content/gdrive/MyDrive/SIS420/Examen_Final_-_SIS420/Pregunta N°1 /Dataset/dataset_17.csv')"
      ],
      "metadata": {
        "id": "OCajlsXJxKHo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = normalizarCaracteristicas(dataset)"
      ],
      "metadata": {
        "id": "qbh-p0DtXj-9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se busca un número óptimo de grupos para luego crear un KMeans y realizar el procesamiento.\n",
        "Este proceso puede tardar incluso una hora"
      ],
      "metadata": {
        "id": "9GEEGUmSlC-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_optimo = encontrar_numero_optimo_grupos(dataset, 10)"
      ],
      "metadata": {
        "id": "VUqsrtixe0aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7e6W_M3gLT-"
      },
      "outputs": [],
      "source": [
        "kmeans_optimo = KMeans(n_clusters=k_optimo, random_state=42, n_init=10)\n",
        "kmeans_optimo.fit(dataset)\n",
        "graficarClusters(dataset.values)\n",
        "plt.show()\n",
        "graficarFronterasDecision(kmeans_optimo, dataset.values)\n",
        "plt.show()\n",
        "y = kmeans_optimo.labels_\n",
        "dataset['labels'] = y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "QlQ8hcwlvkv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(dataset['labels']))\n",
        "print(np.unique(dataset['10W']))"
      ],
      "metadata": {
        "id": "k2-1CkoqwiUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba del Dataset con sus respectivas etiquetas.**"
      ],
      "metadata": {
        "id": "wFjgd5A4G5xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Para este cometido se usará la red neuronal del Laboratorio N°5, por lo que los pasos no requieren estar explicados.*"
      ],
      "metadata": {
        "id": "KHizca_kHCjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelOnevsAll:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.train_set, self.test_set = train_test_split(self.dataset, test_size=0.2, random_state=99)\n",
        "        self.X_train, self.y_train = self.train_set.iloc[:, :-1], self.train_set.iloc[:, -1]\n",
        "        self.X_test, self.y_test = self.test_set.iloc[:, :-1], self.test_set.iloc[:, -1]\n",
        "        self.X_train = np.concatenate([np.ones((self.X_train.shape[0], 1)), self.X_train], axis=1)\n",
        "        self.X_test = np.concatenate([np.ones((self.X_test.shape[0], 1)), self.X_test], axis=1)\n",
        "        self.J = 0\n",
        "    def prepocesarDataset(self, dataset):\n",
        "      dataset = pd.read_csv(dataset, sep=';', header=0, decimal=',')\n",
        "      datos = {}\n",
        "      columnas = dataset.columns[dataset.dtypes == 'object'].tolist()\n",
        "      for columna in columnas:\n",
        "        datos[columna] = dataset[columna].drop_duplicates().values\n",
        "      datos_num = {}\n",
        "      for columna, valores in datos.items():\n",
        "        indice_reemp = 0\n",
        "        datos_num_col = {}\n",
        "        for valor in valores:\n",
        "          if valor not in datos_num_col and not pd.isnull(valor):\n",
        "            datos_num_col[valor] = indice_reemp\n",
        "            indice_reemp += 1\n",
        "        if np.nan not in datos_num_col:\n",
        "          datos_num_col[np.nan] = 0\n",
        "        datos_num[columna] = datos_num_col\n",
        "      for columna, d_n in datos_num.items():\n",
        "        dataset[columna] = dataset[columna].replace(d_n)\n",
        "      dataset = dataset.fillna(0)\n",
        "      return dataset\n",
        "    def normalizarCaracteristicas(self, X):\n",
        "      media = np.mean(X, axis=0)\n",
        "      desviacion_estandar = np.std(X, axis=0)\n",
        "      desviacion_estandar[desviacion_estandar == 0] = 1\n",
        "      X_norm = (X - media) / desviacion_estandar\n",
        "      return X_norm\n",
        "    def graficarCosto(self, J):\n",
        "      plt.plot(np.arange(len(J)), J, lw=2)\n",
        "      plt.xlabel('Numero de iteraciones')\n",
        "      plt.ylabel('Costo J')\n",
        "    def calcularPorcentajeAcierto(self):\n",
        "      predicciones = self.realizarPredicciones(self.X_test, self.theta)\n",
        "      predicciones_redondeadas = np.round(predicciones)\n",
        "      precision = np.mean(predicciones_redondeadas == self.y_test) * 100\n",
        "      print(\"Precisión de las predicciones en el conjunto de prueba: {:.10f}%\".format(precision))\n",
        "    def entrenar(self, regularizacion):\n",
        "        self.lambda_ = 0.01 if regularizacion else 0\n",
        "        self.X_test = self.normalizarCaracteristicas(self.X_test)\n",
        "        self.X_train = self.normalizarCaracteristicas(self.X_train)\n",
        "        self.theta = self.descensoGradiente(self.X_train, self.y_train, self.lambda_, 1000)\n",
        "    def funcionSigmoide(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def funcionCosto(self, theta, X, y, lambda_):\n",
        "      m = y.size\n",
        "      h = self.funcionSigmoide(X @ theta)\n",
        "      costo = (1 / m) * np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
        "      if lambda_ != 0:\n",
        "        regularizacion = (lambda_ / (2 * m)) * np.sum(theta[1:] ** 2)\n",
        "        costo += regularizacion\n",
        "      gradiente = (1 / m) * (X.T @ (h - y))\n",
        "      if lambda_ != 0:\n",
        "        regularizacion_gradiente = (lambda_ / m) * theta\n",
        "        regularizacion_gradiente[0] = 0\n",
        "        gradiente += regularizacion_gradiente\n",
        "      return costo, gradiente\n",
        "    def descensoGradiente(self, X, y, lambda_, num_iteraciones):\n",
        "      m, n = X.shape\n",
        "      num_labels = len(np.unique(y))\n",
        "      theta = np.zeros((num_labels, n))\n",
        "      for c in np.arange(num_labels):\n",
        "        initial_theta = np.zeros(n)\n",
        "        y_onevsall = (y == c).astype(int)\n",
        "        cost_function = lambda t, X, y, lambda_: self.funcionCosto(t, X, y, lambda_)[0]\n",
        "        grad_function = lambda t, X, y, lambda_: self.funcionCosto(t, X, y, lambda_)[1]\n",
        "        res = optimize.minimize(cost_function, initial_theta, args=(X, y_onevsall, lambda_), jac=grad_function, method='CG', options={'maxiter': num_iteraciones})\n",
        "        theta[c] = res.x\n",
        "      return theta\n",
        "    def realizarPredicciones(self, X, theta):\n",
        "        m = X.shape[0]\n",
        "        probabilidades = self.funcionSigmoide(X @ theta.T)\n",
        "        predicciones = np.argmax(probabilidades, axis=1)\n",
        "        return predicciones\n",
        "    def calcularPorcentajeAcierto(self):\n",
        "        predicciones = self.realizarPredicciones(self.X_test, self.theta)\n",
        "        predicciones_redondeadas = np.round(predicciones)\n",
        "        precision = np.mean(predicciones_redondeadas == self.y_test) * 100\n",
        "        print(\"Precisión de las predicciones en el conjunto de prueba: {:.10f}%\".format(precision))"
      ],
      "metadata": {
        "id": "o3IOYcTQ8DcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetOnevsAll = dataset\n",
        "oSR = ModelOnevsAll(datasetOnevsAll)\n",
        "oSR.entrenar(False)"
      ],
      "metadata": {
        "id": "1LSGliKJ8Eh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oSR.calcularPorcentajeAcierto()"
      ],
      "metadata": {
        "id": "G7umcNRU8JhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Nota:*\n",
        "El tiempo de ejecución puede extenderse hasta una hora debido a varios factores:\n",
        "1. **Complejidad del algoritmo K-Means**: K-Means tiene una complejidad de O(n * k * d * i), donde n es el número de puntos de datos, k es el número de clusters, d es el número de dimensiones y i es el número de iteraciones.\n",
        "2. **Dimensionalidad del conjunto de datos**: A mayor número de características (dimensiones), más tiempo requiere el algoritmo para calcular las distancias y asignar los clusters.\n",
        "3. **Número de iteraciones**: El algoritmo puede necesitar muchas iteraciones para converger a una solución óptima, especialmente en datos complejos.\n",
        "4. **Preprocesamiento de datos**: Incluye carga, limpieza y escalado de datos, lo que puede consumir tiempo significativo."
      ],
      "metadata": {
        "id": "wle4uTGXkZxa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}